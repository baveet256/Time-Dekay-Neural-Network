{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-25T08:10:47.776406Z","iopub.execute_input":"2023-04-25T08:10:47.776806Z","iopub.status.idle":"2023-04-25T08:10:47.800932Z","shell.execute_reply.started":"2023-04-25T08:10:47.776771Z","shell.execute_reply":"2023-04-25T08:10:47.799996Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\n\nimport os, pathlib, glob, random\nimport numpy as np\nimport matplotlib.pyplot as plt \n\nfrom sklearn.metrics import confusion_matrix,f1_score,jaccard_score,matthews_corrcoef,hamming_loss\nimport scipy","metadata":{"execution":{"iopub.status.busy":"2023-04-25T08:10:47.802450Z","iopub.execute_input":"2023-04-25T08:10:47.802786Z","iopub.status.idle":"2023-04-25T08:10:50.649380Z","shell.execute_reply.started":"2023-04-25T08:10:47.802750Z","shell.execute_reply":"2023-04-25T08:10:50.648336Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T08:10:50.650913Z","iopub.execute_input":"2023-04-25T08:10:50.651434Z","iopub.status.idle":"2023-04-25T08:10:50.708713Z","shell.execute_reply.started":"2023-04-25T08:10:50.651395Z","shell.execute_reply":"2023-04-25T08:10:50.707544Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 32\noutput_nodes = 5\nlearning_rate = 0.01\ntrain_data_path = r\"/kaggle/input/lfrcc-order16-5200t/order_16-split/train\"\ntest_data_path = r\"/kaggle/input/lfrcc-order16-5200t/order_16-split/test\"","metadata":{"execution":{"iopub.status.busy":"2023-04-25T08:10:50.712855Z","iopub.execute_input":"2023-04-25T08:10:50.713764Z","iopub.status.idle":"2023-04-25T08:10:50.719198Z","shell.execute_reply.started":"2023-04-25T08:10:50.713726Z","shell.execute_reply":"2023-04-25T08:10:50.718069Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class PtDataset(Dataset):\n    def __init__(self, directory):\n        self.directory = directory\n        self.classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n        self.files = []\n        for c in self.classes:\n            c_dir = os.path.join(directory, c)\n            c_files = [(os.path.join(c_dir, f), self.class_to_idx[c]) for f in os.listdir(c_dir)]\n            self.files.extend(c_files)\n        random.shuffle(self.files)\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, idx):\n        filepath, label = self.files[idx]\n        try:\n            mat_vals = scipy.io.loadmat(filepath)\n            data = mat_vals['final']\n            max_len=1332\n            if (max_len > data.shape[0]):\n                pad_width = max_len - data.shape[0]\n                data = np.pad(data, pad_width=((0, pad_width),(0,0)), mode='constant')\n            else:\n                data = data[:max_len, :]\n        except Exception as e:\n            print(f\"Error loading file {filepath}: {str(e)}\")\n            return None\n        return data, label\n    \ntrain_dataset = PtDataset(train_data_path)\ntest_dataset = PtDataset(test_data_path)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T08:10:50.721000Z","iopub.execute_input":"2023-04-25T08:10:50.721493Z","iopub.status.idle":"2023-04-25T08:10:51.550103Z","shell.execute_reply.started":"2023-04-25T08:10:50.721457Z","shell.execute_reply":"2023-04-25T08:10:51.548991Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class PtDataLoader(DataLoader):\n    def __init__(self, directory, batch_size, shuffle=True):\n        dataset = PtDataset(directory)\n        super().__init__(dataset, batch_size=batch_size, shuffle=shuffle)\n        \ntrain_dataloader = PtDataLoader(directory=train_data_path, batch_size=batch_size)\ntest_dataloader = PtDataLoader(directory=test_data_path, batch_size=batch_size)\n\ntrain_count = len(train_dataset) \ntest_count = len(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T08:10:51.553591Z","iopub.execute_input":"2023-04-25T08:10:51.553941Z","iopub.status.idle":"2023-04-25T08:10:51.630068Z","shell.execute_reply.started":"2023-04-25T08:10:51.553911Z","shell.execute_reply":"2023-04-25T08:10:51.629116Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class TDNN(nn.Module):\n    \n    def __init__(\n                    self, \n                    input_dim, \n                    output_dim,\n                    context_size=5,\n                    stride=1,\n                    dilation=1,\n                    batch_norm=False,\n                    dropout_p=0.2\n                ):\n        super(TDNN, self).__init__()\n        self.context_size = context_size\n        self.stride = stride\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.dilation = dilation\n        self.dropout_p = dropout_p\n        self.batch_norm = batch_norm\n      \n        self.kernel = nn.Linear(input_dim*context_size, output_dim)\n        self.nonlinearity = nn.ReLU()\n        if self.batch_norm:\n            self.bn = nn.BatchNorm1d(output_dim)\n        if self.dropout_p:\n            self.drop = nn.Dropout(p=self.dropout_p)\n        \n    def forward(self, x):\n        '''\n        input: size (batch, seq_len, input_features)\n        outpu: size (batch, new_seq_len, output_features)\n        '''\n        \n        _, _, d = x.shape\n        assert (d == self.input_dim), 'Input dimension was wrong. Expected ({}), got ({})'.format(self.input_dim, d)\n        x = x.unsqueeze(1)\n\n        # Unfold input into smaller temporal contexts\n        x = F.unfold(\n                        x, \n                        (self.context_size, self.input_dim), \n                        stride=(1,self.input_dim), \n                        dilation=(self.dilation,1)\n                    )\n\n        # N, output_dim*context_size, new_t = x.shape\n        x = x.transpose(1,2)\n        x = self.kernel(x.float())\n        x = self.nonlinearity(x)\n        \n        if self.dropout_p:\n            x = self.drop(x)\n\n        if self.batch_norm:\n            x = x.transpose(1,2)\n            x = self.bn(x)\n            x = x.transpose(1,2)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-04-25T08:10:51.632911Z","iopub.execute_input":"2023-04-25T08:10:51.633611Z","iopub.status.idle":"2023-04-25T08:10:51.645240Z","shell.execute_reply.started":"2023-04-25T08:10:51.633573Z","shell.execute_reply":"2023-04-25T08:10:51.643966Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Conv2DLayer(nn.Module):\n    \"\"\"Convolutional 2D subsampling (to 1/4 length).\n    Module accepts an input tensor with size (B, T1, I_DIM),\n    then subsamples it into tensor with size (B, T2, O_DIM)\n    in which T2 is about T1 / 4.\n    :param i_dim: Dimension of input feature\n    :type i_dim: int\n    :param o_dim: Dimension of output feature\n    :type o_dim: int\n    :param dropout_rate: Dropout rate\n    :type o_dim: float\n    \"\"\"\n    def __init__(\n        self,\n        i_dim: int,\n        o_dim: int,\n    ) -> None:\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, o_dim, 3, 2),\n            nn.ReLU(),\n            nn.Conv2d(o_dim, o_dim, 3, 2),\n            nn.ReLU(),\n        )\n        # after convolution, tensor size should be (b, o_dim, t, f_dim)\n        f_dim = (i_dim - 3) // 2 + 1 ## operation to determine the shape of the output after the 1st convolution layer\n        f_dim = (f_dim - 3) // 2 + 1 ## operation to determine the shape of the output after the 2nd convolution layer\n        self.out = nn.Linear(o_dim * f_dim, o_dim)\n\n    def forward(self, x):\n        x=x.float()\n        x = x.unsqueeze(1)  # (b, t, i_dim) -> (b, 1, t, i_dim)\n        x = self.conv(x)  # (b, 1, t, i_dim) -> (b, o_dim, t/4, f_dim)\n#         print(x.shape)\n        b, c, t, f = x.size()\n        x = x.transpose(1, 2).contiguous().view(b, t, c * f)\n#         print(x.shape)\n        x = self.out(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-04-25T08:10:51.648370Z","iopub.execute_input":"2023-04-25T08:10:51.648685Z","iopub.status.idle":"2023-04-25T08:10:51.659475Z","shell.execute_reply.started":"2023-04-25T08:10:51.648649Z","shell.execute_reply":"2023-04-25T08:10:51.658355Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# model_test = Conv2DLayer(39,39).to(device)\n# y = torch.tensor(np.random.rand(32,1332,39)).to(device)\n# p=model_test(y)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T08:10:51.660653Z","iopub.execute_input":"2023-04-25T08:10:51.661488Z","iopub.status.idle":"2023-04-25T08:10:51.670732Z","shell.execute_reply.started":"2023-04-25T08:10:51.661449Z","shell.execute_reply":"2023-04-25T08:10:51.669786Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\n\nclass Classic_Attention(nn.Module):\n    def __init__(self,input_dim, embed_dim, attn_dropout=0.0):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.attn_dropout = attn_dropout\n        self.lin_proj = nn.Linear(input_dim,embed_dim)\n        self.v = torch.nn.Parameter(torch.randn(embed_dim))\n    \n    def forward(self,inputs):\n        lin_out = self.lin_proj(inputs)\n        v_view = self.v.unsqueeze(0).expand(lin_out.size(0), len(self.v)).unsqueeze(2)\n        attention_weights = torch.tanh(lin_out.bmm(v_view).squeeze(2))\n        attention_weights_normalized = F.softmax(attention_weights,1)\n        return attention_weights_normalized","metadata":{"execution":{"iopub.status.busy":"2023-04-25T08:10:51.674854Z","iopub.execute_input":"2023-04-25T08:10:51.675133Z","iopub.status.idle":"2023-04-25T08:10:51.683207Z","shell.execute_reply.started":"2023-04-25T08:10:51.675108Z","shell.execute_reply":"2023-04-25T08:10:51.681868Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class Convnet(nn.Module):\n    def __init__(self, input_dim = 39, num_classes=5):\n        super(Convnet, self).__init__()\n        ## Frame level feature processing\n        self.conv_subsample = Conv2DLayer(39,39)\n        self.tdnn1 = TDNN(input_dim=39, output_dim=512, context_size=5, dilation=1,dropout_p=0.2)\n        self.tdnn2 = TDNN(input_dim=512, output_dim=512, context_size=5, dilation=2,dropout_p=0.2)\n        self.tdnn3 = TDNN(input_dim=512, output_dim=512, context_size=7, dilation=3,dropout_p=0.2)\n        self.tdnn4 = TDNN(input_dim=512, output_dim=512, context_size=1, dilation=1,dropout_p=0.2)\n        self.tdnn5 = TDNN(input_dim=512, output_dim=512, context_size=1, dilation=1,dropout_p=0.2)\n        ### Statistics attentive pooling\n        self.attention = Classic_Attention(512,512)\n        #### Frame levelPooling\n        self.segment6 = nn.Linear(1024, 512)\n        self.segment7 = nn.Linear(512,512 )\n        self.output = nn.Linear(512, num_classes)\n        self.softmax = nn.Softmax(dim=1)\n    \n    def weighted_sd(self,inputs,attention_weights, mean):\n        el_mat_prod = torch.mul(inputs,attention_weights.unsqueeze(2).expand(-1,-1,inputs.shape[-1]))\n        hadmard_prod = torch.mul(inputs,el_mat_prod)\n        variance = torch.sum(hadmard_prod,1) - torch.mul(mean,mean)\n        return variance\n    \n    \n    def stat_attn_pool(self,inputs,attention_weights):\n        el_mat_prod = torch.mul(inputs,attention_weights.unsqueeze(2).expand(-1,-1,inputs.shape[-1]))\n        mean = torch.mean(el_mat_prod,1)\n        variance = self.weighted_sd(inputs,attention_weights,mean)\n        stat_pooling = torch.cat((mean,variance),1)\n        return stat_pooling\n    \n    \n    def forward(self, inputs):\n        inputs=self.conv_subsample(inputs)\n#         print(inputs.shape)\n        tdnn1_out = self.tdnn1(inputs)\n        tdnn2_out = self.tdnn2(tdnn1_out)\n        tdnn3_out = self.tdnn3(tdnn2_out)\n        tdnn4_out = self.tdnn4(tdnn3_out)\n        tdnn5_out = self.tdnn5(tdnn4_out)\n        ### Stat Pool\n        attn_weights = self.attention(tdnn5_out)\n        stat_pool_out = self.stat_attn_pool(tdnn5_out,attn_weights)\n        segment6_out = self.segment6(stat_pool_out)\n        x_vec = self.segment7(segment6_out)\n        predictions = self.output(x_vec)\n        return predictions,x_vec","metadata":{"execution":{"iopub.status.busy":"2023-04-25T08:10:51.685081Z","iopub.execute_input":"2023-04-25T08:10:51.685916Z","iopub.status.idle":"2023-04-25T08:10:51.700426Z","shell.execute_reply.started":"2023-04-25T08:10:51.685863Z","shell.execute_reply":"2023-04-25T08:10:51.699381Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model = Convnet().to(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T08:10:51.702067Z","iopub.execute_input":"2023-04-25T08:10:51.702500Z","iopub.status.idle":"2023-04-25T08:10:54.139196Z","shell.execute_reply.started":"2023-04-25T08:10:51.702464Z","shell.execute_reply":"2023-04-25T08:10:54.138173Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# y = torch.tensor(np.random.rand(32,1332,39)).to(device)\n# p=model(y)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T08:10:54.140553Z","iopub.execute_input":"2023-04-25T08:10:54.141014Z","iopub.status.idle":"2023-04-25T08:10:54.145665Z","shell.execute_reply.started":"2023-04-25T08:10:54.140976Z","shell.execute_reply":"2023-04-25T08:10:54.144658Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# p[0].shape","metadata":{"execution":{"iopub.status.busy":"2023-04-25T08:10:54.146953Z","iopub.execute_input":"2023-04-25T08:10:54.147606Z","iopub.status.idle":"2023-04-25T08:10:54.156315Z","shell.execute_reply.started":"2023-04-25T08:10:54.147516Z","shell.execute_reply":"2023-04-25T08:10:54.155075Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"loss_function = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T08:10:54.159454Z","iopub.execute_input":"2023-04-25T08:10:54.159783Z","iopub.status.idle":"2023-04-25T08:10:54.166917Z","shell.execute_reply.started":"2023-04-25T08:10:54.159756Z","shell.execute_reply":"2023-04-25T08:10:54.165681Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# learning_rate=0.001\n# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T08:10:54.168357Z","iopub.execute_input":"2023-04-25T08:10:54.169491Z","iopub.status.idle":"2023-04-25T08:10:54.174141Z","shell.execute_reply.started":"2023-04-25T08:10:54.169453Z","shell.execute_reply":"2023-04-25T08:10:54.172971Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"%pip install tqdm\nfrom tqdm import tqdm\nimport scipy\nfrom scipy import io\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T08:10:54.175530Z","iopub.execute_input":"2023-04-25T08:10:54.176693Z","iopub.status.idle":"2023-04-25T08:11:04.642649Z","shell.execute_reply.started":"2023-04-25T08:10:54.176649Z","shell.execute_reply":"2023-04-25T08:11:04.641355Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.64.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nConvnet(\n  (conv_subsample): Conv2DLayer(\n    (conv): Sequential(\n      (0): Conv2d(1, 39, kernel_size=(3, 3), stride=(2, 2))\n      (1): ReLU()\n      (2): Conv2d(39, 39, kernel_size=(3, 3), stride=(2, 2))\n      (3): ReLU()\n    )\n    (out): Linear(in_features=351, out_features=39, bias=True)\n  )\n  (tdnn1): TDNN(\n    (kernel): Linear(in_features=195, out_features=512, bias=True)\n    (nonlinearity): ReLU()\n    (drop): Dropout(p=0.2, inplace=False)\n  )\n  (tdnn2): TDNN(\n    (kernel): Linear(in_features=2560, out_features=512, bias=True)\n    (nonlinearity): ReLU()\n    (drop): Dropout(p=0.2, inplace=False)\n  )\n  (tdnn3): TDNN(\n    (kernel): Linear(in_features=3584, out_features=512, bias=True)\n    (nonlinearity): ReLU()\n    (drop): Dropout(p=0.2, inplace=False)\n  )\n  (tdnn4): TDNN(\n    (kernel): Linear(in_features=512, out_features=512, bias=True)\n    (nonlinearity): ReLU()\n    (drop): Dropout(p=0.2, inplace=False)\n  )\n  (tdnn5): TDNN(\n    (kernel): Linear(in_features=512, out_features=512, bias=True)\n    (nonlinearity): ReLU()\n    (drop): Dropout(p=0.2, inplace=False)\n  )\n  (attention): Classic_Attention(\n    (lin_proj): Linear(in_features=512, out_features=512, bias=True)\n  )\n  (segment6): Linear(in_features=1024, out_features=512, bias=True)\n  (segment7): Linear(in_features=512, out_features=512, bias=True)\n  (output): Linear(in_features=512, out_features=5, bias=True)\n  (softmax): Softmax(dim=1)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"#Model training and testing \nn_total_steps = len(train_dataloader) # n_total_steps * batch size will give total number of training files (consider that last batch may not be fully filled)\ntrain_accuracy_list = []\ntrain_loss_list = []\ntest_accuracy_list = []\npred_labels =[]\nact_labels = []\nmax_acc=0\nnum_epochs = 10\nfor epoch in tqdm(range(num_epochs)):\n    \n    #Evaluation and training on training dataset\n    model.train()\n    train_accuracy=0.0\n    train_loss=0.0\n    \n    for batch_idx, (images,labels) in enumerate(train_dataloader):\n        if torch.cuda.is_available():\n            images=Variable(images.cuda())\n            labels=Variable(labels.cuda())\n        ##images = images.unsqueeze(1)\n        optimizer.zero_grad()\n        outputs=model(images)\n        outputs=outputs[0]\n#         labels = labels.float()\n#         labels=labels.unsqueeze(1)\n        loss=loss_function(outputs,labels)\n        loss.backward()\n        optimizer.step()\n        \n        \n        train_loss+= loss.cpu().data*images.size(0)\n        _,prediction=torch.max(outputs.data,1)\n        \n        train_accuracy+=int(torch.sum(prediction==labels.data))\n        \n    train_accuracy=train_accuracy/train_count\n    train_loss=train_loss/train_count\n    \n    train_accuracy_list.append(train_accuracy)\n    train_loss_list.append(train_loss)\n\n    \n    # Evaluation on testing dataset\n    model.eval()\n    test_accuracy=0.0\n    pred = []\n    lab = []\n    for i, (images,labels) in enumerate(test_dataloader):\n        if torch.cuda.is_available():\n            images=Variable(images.cuda())\n            labels=Variable(labels.cuda())\n        ##images = images.unsqueeze(1) \n#         print(i,images.shape)\n        outputs=model(images)\n        outputs=outputs[0]\n#         labels = labels.float()\n#         labels=labels.unsqueeze(1)\n        _,prediction=torch.max(outputs.data,1)\n        test_accuracy+=int(torch.sum(prediction==labels.data))\n        pred.extend(prediction.tolist())\n        lab.extend(labels.tolist())\n    test_accuracy=test_accuracy/test_count\n    test_accuracy_list.append(test_accuracy)\n    if max_acc < test_accuracy:\n        max_acc = test_accuracy\n        pred_labels = pred\n        actual_labels = lab\n        torch.save(model,'best_save_lfrcc_tdnn_conv-sample-1332.pth')\n    \n    print('Epoch : '+str(epoch+1)+'/'+str(num_epochs)+'   Train Loss : '+str(train_loss)+'   Train Accuracy : '+str(train_accuracy)+'   Test Accuracy : '+str(test_accuracy))\nprint(max_acc)    \nprint('Finished Training and Testing')","metadata":{"execution":{"iopub.status.busy":"2023-04-25T08:11:22.898779Z","iopub.execute_input":"2023-04-25T08:11:22.899906Z","iopub.status.idle":"2023-04-25T08:38:09.941071Z","shell.execute_reply.started":"2023-04-25T08:11:22.899856Z","shell.execute_reply":"2023-04-25T08:38:09.938383Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":" 10%|█         | 1/10 [04:07<37:04, 247.21s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch : 1/10   Train Loss : tensor(1.6106)   Train Accuracy : 0.20009640877319837   Test Accuracy : 0.2004626060138782\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 2/10 [06:38<25:27, 190.99s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch : 2/10   Train Loss : tensor(1.6104)   Train Accuracy : 0.2   Test Accuracy : 0.1981495759444873\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 3/10 [09:10<20:12, 173.20s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch : 3/10   Train Loss : tensor(1.6103)   Train Accuracy : 0.19956616052060738   Test Accuracy : 0.2004626060138782\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 4/10 [11:41<16:25, 164.32s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch : 4/10   Train Loss : tensor(1.6105)   Train Accuracy : 0.1990359122680164   Test Accuracy : 0.2004626060138782\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 5/10 [14:12<13:18, 159.66s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch : 5/10   Train Loss : tensor(1.6106)   Train Accuracy : 0.1975415762834418   Test Accuracy : 0.1981495759444873\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 6/10 [16:45<10:28, 157.17s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch : 6/10   Train Loss : tensor(1.6105)   Train Accuracy : 0.1977825982164377   Test Accuracy : 0.2004626060138782\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 7/10 [19:15<07:44, 154.92s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch : 7/10   Train Loss : tensor(1.6105)   Train Accuracy : 0.1970113280308508   Test Accuracy : 0.2004626060138782\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 8/10 [21:46<05:06, 153.49s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch : 8/10   Train Loss : tensor(1.6104)   Train Accuracy : 0.19734875873704508   Test Accuracy : 0.2004626060138782\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 9/10 [24:16<02:32, 152.58s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch : 9/10   Train Loss : tensor(1.6104)   Train Accuracy : 0.19865027717522293   Test Accuracy : 0.2004626060138782\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [26:47<00:00, 160.70s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch : 10/10   Train Loss : tensor(1.6099)   Train Accuracy : 0.20086767895878524   Test Accuracy : 0.2004626060138782\n0.2004626060138782\nFinished Training and Testing\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}